{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell started at: 03:50:54\n",
      "Using 1000 trials, max_d=12, M=4.8855\n",
      "Number of available threads: 64\n",
      "\n",
      "--- Warm-up direct loglik ---\n",
      "\n",
      "--- Warm-up PyTensor Op ---\n",
      "Direct loglik(true)      = -1493.814546\n",
      "LogLikeFPT loglik(true)  = -1493.814546\n",
      "\n",
      "--- Warm-up full PyMC logp ---\n",
      "Full PyMC logp(initial_point) = -1563.840746\n",
      "\n",
      "=== Timing log-likelihood / logp evaluations ===\n",
      "Direct loglik:       5.584 ms per call\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTensor Op:         5.585 ms per call (1.00× vs direct)\n",
      "Full PyMC logp:      5.493 ms per call (0.98× vs direct, 0.98× vs Op)\n",
      "\n",
      "=== Benchmark pm.sample (with full timing) ===\n",
      "Calling pm.sample at: 03:50:54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [eta]\n",
      ">Metropolis: [kappa]\n",
      ">Metropolis: [a]\n",
      ">Metropolis: [b_raw]\n",
      ">Metropolis: [x0_raw]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8472fd2d780d4b0a83ce5a7eafc9d81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 1 chain for 0 tune and 100 draw iterations (0 + 100 draws total) took 6 seconds.\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm.sample returned at: 03:51:01\n",
      "pm.sample wall time (sample_t1 - sample_t0): 7.1 s\n",
      "pm.sample: 100 draws => 70.6 ms per draw\n",
      "Bench chain overall accept rate: [0.2 0.4 0.4 0.2 0.2 0.4 0.2 0.2 0.2 0.2 0.  0.  0.  0.  0.  0.2 0.  0.\n",
      " 0.  0.  0.  0.  0.  0.2 0.  0.  0.2 0.2 0.2 0.  0.  0.  0.2 0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  0.  0.4 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 0.  0.2 0.  0.4 0.  0.4 0.  0.  0.2 0.  0.4 0.  0.  0.  0.  0.  0.  0.\n",
      " 0.2 0.2 0.  0.  0.2 0.  0.  0.4 0.  0.  0.  0.  0.2 0.2 0.  0.  0.  0.\n",
      " 0.  0.  0.2 0.  0.  0.  0.  0.2 0.  0. ]\n",
      "Cell finished at: 03:51:01\n",
      "Full cell wall time (cell_t1 - cell_t0): 7.6 s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "\n",
    "from pytensor.graph.op import Op\n",
    "from efficient_fpt.multi_stage_cy import compute_loss_parallel, print_num_threads\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# Config\n",
    "# =====================================================================\n",
    "DATA_PATH      = \"addm_data_20251015-163921.pkl\"\n",
    "START_INDEX    = 0\n",
    "END_INDEX      = 1000   # adjust if you want fewer trials\n",
    "NUM_THREADS    = 32      # threads for compute_loss_parallel\n",
    "N_TIMES        = 20      # repetitions for timing loglik/logp\n",
    "N_DRAWS_BENCH  = 100     # draws for pm.sample benchmark\n",
    "\n",
    "# Optional: keep BLAS single-threaded so OpenMP (your Cython) dominates\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# Helper: timing function\n",
    "# =====================================================================\n",
    "def time_func(func, *args, n=20):\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(n):\n",
    "        func(*args)\n",
    "    t1 = time.perf_counter()\n",
    "    return (t1 - t0) / n\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# Start full-cell timer\n",
    "# =====================================================================\n",
    "cell_t0 = time.perf_counter()\n",
    "print(\"Cell started at:\", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# Load data\n",
    "# =====================================================================\n",
    "data = pickle.load(open(DATA_PATH, \"rb\"))\n",
    "\n",
    "DATA_TYPE = np.float64\n",
    "\n",
    "# True parameters\n",
    "a_true     = float(data[\"a\"])\n",
    "b_true     = float(data[\"b\"])\n",
    "x0_true    = float(data[\"x0\"])\n",
    "eta_true   = float(data[\"eta\"])\n",
    "kappa_true = float(data[\"kappa\"])\n",
    "\n",
    "r1_full    = data[\"r1_data\"]\n",
    "r2_full    = data[\"r2_data\"]\n",
    "flag_full  = data[\"flag_data\"].astype(np.int32)\n",
    "\n",
    "sigma = float(data[\"sigma\"])\n",
    "T     = float(data[\"T\"])\n",
    "\n",
    "mu_full     = data[\"mu_array_padded_data\"].astype(DATA_TYPE)\n",
    "sacc_full   = data[\"sacc_array_padded_data\"].astype(DATA_TYPE)\n",
    "length_full = data[\"d_data\"].astype(np.int32)\n",
    "rt_full     = data[\"decision_data\"][:, 0].astype(DATA_TYPE)\n",
    "choice_full = data[\"decision_data\"][:, 1].astype(np.int32)\n",
    "\n",
    "num_data_full, max_d = mu_full.shape\n",
    "\n",
    "# Subset trials\n",
    "start_index = START_INDEX\n",
    "end_index   = min(END_INDEX, num_data_full)\n",
    "idx         = slice(start_index, end_index)\n",
    "\n",
    "r1_data     = r1_full[idx]\n",
    "r2_data     = r2_full[idx]\n",
    "flag_data   = flag_full[idx]\n",
    "sacc_data   = sacc_full[idx]\n",
    "length_data = length_full[idx]\n",
    "rt_data     = rt_full[idx]\n",
    "choice_data = choice_full[idx]\n",
    "\n",
    "num_data = len(rt_data)\n",
    "M = float(np.max(rt_data))\n",
    "\n",
    "print(f\"Using {num_data} trials, max_d={max_d}, M={M:.4f}\")\n",
    "print_num_threads()\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# Direct log-likelihood (baseline)\n",
    "# =====================================================================\n",
    "def loglik_direct(eta, kappa, a, b, x0, num_threads=NUM_THREADS):\n",
    "    \"\"\"Direct call to compute_loss_parallel, as in your custom code.\"\"\"\n",
    "    mu1 = kappa * (r1_data - eta * r2_data)\n",
    "    mu2 = kappa * (eta * r1_data - r2_data)\n",
    "    nll = compute_loss_parallel(\n",
    "        mu1,\n",
    "        mu2,\n",
    "        rt_data,\n",
    "        choice_data,\n",
    "        flag_data,\n",
    "        sacc_data,\n",
    "        length_data,\n",
    "        max_d,\n",
    "        sigma,\n",
    "        a,\n",
    "        b,\n",
    "        x0,\n",
    "        num_threads=num_threads,\n",
    "    )\n",
    "    return -num_data * nll\n",
    "\n",
    "\n",
    "theta_true_vec = np.array(\n",
    "    [eta_true, kappa_true, a_true, b_true, x0_true], dtype=float\n",
    ")\n",
    "eta0, kappa0, a0, b0, x00 = theta_true_vec\n",
    "\n",
    "print(\"\\n--- Warm-up direct loglik ---\")\n",
    "_ = loglik_direct(eta0, kappa0, a0, b0, x00)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# PyTensor Op for the log-likelihood\n",
    "# =====================================================================\n",
    "class LogLikeFPT(Op):\n",
    "    \"\"\"\n",
    "    PyTensor Op that wraps compute_loss_parallel.\n",
    "    Input: theta = [eta, kappa, a, b, x0] (1D np array)\n",
    "    Output: scalar log-likelihood.\n",
    "    \"\"\"\n",
    "    itypes = [pt.dvector]\n",
    "    otypes = [pt.dscalar]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rt_data,\n",
    "        choice_data,\n",
    "        r1_data,\n",
    "        r2_data,\n",
    "        flag_data,\n",
    "        sacc_data,\n",
    "        length_data,\n",
    "        max_d,\n",
    "        sigma,\n",
    "        num_threads=NUM_THREADS,\n",
    "    ):\n",
    "        self.rt_data     = np.asarray(rt_data, dtype=np.float64)\n",
    "        self.choice_data = np.asarray(choice_data, dtype=np.int32)\n",
    "        self.r1_data     = np.asarray(r1_data, dtype=np.float64)\n",
    "        self.r2_data     = np.asarray(r2_data, dtype=np.float64)\n",
    "        self.flag_data   = np.asarray(flag_data, dtype=np.int32)\n",
    "        self.sacc_data   = np.asarray(sacc_data, dtype=np.float64)\n",
    "        self.length_data = np.asarray(length_data, dtype=np.int32)\n",
    "\n",
    "        self.max_d = int(max_d)\n",
    "        self.sigma = float(sigma)\n",
    "        self.num_data = len(self.rt_data)\n",
    "        self.num_threads = int(num_threads)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        (theta,) = inputs\n",
    "        eta, kappa, a, b, x0 = theta\n",
    "\n",
    "        mu1_data = kappa * (self.r1_data - eta * self.r2_data)\n",
    "        mu2_data = kappa * (eta * self.r1_data - self.r2_data)\n",
    "\n",
    "        nll = compute_loss_parallel(\n",
    "            mu1_data,\n",
    "            mu2_data,\n",
    "            self.rt_data,\n",
    "            self.choice_data,\n",
    "            self.flag_data,\n",
    "            self.sacc_data,\n",
    "            self.length_data,\n",
    "            self.max_d,\n",
    "            self.sigma,\n",
    "            a,\n",
    "            b,\n",
    "            x0,\n",
    "            num_threads=self.num_threads,\n",
    "        )\n",
    "        loglik = -self.num_data * nll\n",
    "        outputs[0][0] = np.array(loglik, dtype=\"float64\")\n",
    "\n",
    "\n",
    "loglike_op = LogLikeFPT(\n",
    "    rt_data=rt_data,\n",
    "    choice_data=choice_data,\n",
    "    r1_data=r1_data,\n",
    "    r2_data=r2_data,\n",
    "    flag_data=flag_data,\n",
    "    sacc_data=sacc_data,\n",
    "    length_data=length_data,\n",
    "    max_d=max_d,\n",
    "    sigma=sigma,\n",
    "    num_threads=NUM_THREADS,\n",
    ")\n",
    "\n",
    "theta_sym = pt.dvector(\"theta_sym\")\n",
    "ll_sym = loglike_op(theta_sym)\n",
    "f_ll = pytensor.function([theta_sym], ll_sym)\n",
    "\n",
    "print(\"\\n--- Warm-up PyTensor Op ---\")\n",
    "_ = f_ll(theta_true_vec)\n",
    "\n",
    "ll_direct = loglik_direct(eta0, kappa0, a0, b0, x00)\n",
    "ll_op     = float(f_ll(theta_true_vec))\n",
    "print(f\"Direct loglik(true)      = {ll_direct:.6f}\")\n",
    "print(f\"LogLikeFPT loglik(true)  = {ll_op:.6f}\")\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# Build PyMC model and compile full logp\n",
    "# =====================================================================\n",
    "with pm.Model() as model:\n",
    "    eta     = pm.Beta(\"eta\", alpha=2.0, beta=2.0)\n",
    "    kappa   = pm.Gamma(\"kappa\", alpha=2.0, beta=4.0)  # scale = 1/4\n",
    "    a_param = pm.Gamma(\"a\", alpha=2.0, beta=1.0)      # scale = 1\n",
    "\n",
    "    b_raw   = pm.Beta(\"b_raw\", alpha=2.0, beta=2.0)\n",
    "    b_param = pm.Deterministic(\"b\", b_raw * a_param / M)\n",
    "\n",
    "    x0_raw   = pm.Beta(\"x0_raw\", alpha=2.0, beta=2.0)\n",
    "    x0_param = pm.Deterministic(\"x0\", -a_param + 2.0 * a_param * x0_raw)\n",
    "\n",
    "    theta = pt.stack([eta, kappa, a_param, b_param, x0_param])\n",
    "    pm.Potential(\"loglik\", loglike_op(theta))\n",
    "\n",
    "    # compile full logp (priors + potential)\n",
    "    logp_fn = model.compile_fn(model.logp(sum=True))\n",
    "    # get a valid starting point in transformed space\n",
    "    ip = model.initial_point()\n",
    "\n",
    "print(\"\\n--- Warm-up full PyMC logp ---\")\n",
    "logp_val = float(logp_fn(ip))\n",
    "print(f\"Full PyMC logp(initial_point) = {logp_val:.6f}\")\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# Timing: direct vs Op vs full logp\n",
    "# =====================================================================\n",
    "print(\"\\n=== Timing log-likelihood / logp evaluations ===\")\n",
    "t_direct = time_func(loglik_direct, eta0, kappa0, a0, b0, x00, NUM_THREADS, n=N_TIMES)\n",
    "print(f\"Direct loglik:       {t_direct*1000:.3f} ms per call\")\n",
    "\n",
    "t_op = time_func(f_ll, theta_true_vec, n=N_TIMES)\n",
    "print(f\"PyTensor Op:         {t_op*1000:.3f} ms per call \"\n",
    "      f\"({t_op/t_direct:.2f}× vs direct)\")\n",
    "\n",
    "t_logp = time_func(logp_fn, ip, n=N_TIMES)\n",
    "print(f\"Full PyMC logp:      {t_logp*1000:.3f} ms per call \"\n",
    "      f\"({t_logp/t_direct:.2f}× vs direct, {t_logp/t_op:.2f}× vs Op)\")\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# Benchmark pm.sample: with *full* timing around it\n",
    "# =====================================================================\n",
    "print(\"\\n=== Benchmark pm.sample (with full timing) ===\")\n",
    "print(\"Calling pm.sample at:\", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "with model:\n",
    "    sample_t0 = time.perf_counter()\n",
    "    trace_bench = pm.sample(\n",
    "        draws=N_DRAWS_BENCH,\n",
    "        tune=0,\n",
    "        chains=1,\n",
    "        cores=1,\n",
    "        step=pm.Metropolis(),\n",
    "        progressbar=True,\n",
    "        return_inferencedata=True,\n",
    "    )\n",
    "    sample_t1 = time.perf_counter()\n",
    "\n",
    "print(\"pm.sample returned at:\", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "print(f\"pm.sample wall time (sample_t1 - sample_t0): {sample_t1 - sample_t0:.1f} s\")\n",
    "print(f\"pm.sample: {N_DRAWS_BENCH} draws => { (sample_t1 - sample_t0)/N_DRAWS_BENCH*1000:.1f} ms per draw\")\n",
    "\n",
    "acc = trace_bench.sample_stats[\"accepted\"].values  # (1, draws, 5)\n",
    "print(\"Bench chain overall accept rate:\", acc.mean(axis=(0, 2)))\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# End full-cell timing\n",
    "# =====================================================================\n",
    "cell_t1 = time.perf_counter()\n",
    "print(\"Cell finished at:\", datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "print(f\"Full cell wall time (cell_t1 - cell_t0): {cell_t1 - cell_t0:.1f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
